#!/bin/bash
#SBATCH --job-name=instructpix2pix-sdxl
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!
#SBATCH --cpus-per-task=96
#SBATCH --gres=gpu:8
#SBATCH --exclusive
#SBATCH --partition=production-cluster
#SBATCH --output=/admin/home/suraj/logs/maskgit-imagenet/%x-%j.out

set -x -e

source /admin/home/suraj/.bashrc
source /fsx/suraj/miniconda3/etc/profile.d/conda.sh
conda activate muse

echo "START TIME: $(date)"

REPO=/admin/home/suraj/code/instructpix2pix-sdxl
OUTPUT_DIR=/fsx/suraj/instructpix2pix-sdxl
LOG_PATH=$OUTPUT_DIR/main_log.txt
ACCELERATE_CONFIG_FILE="$OUTPUT_DIR/${SLURM_JOB_ID}_accelerate_config.yaml.autogenerated"

mkdir -p $OUTPUT_DIR
touch $LOG_PATH
pushd $REPO


GPUS_PER_NODE=8
NNODES=$SLURM_NNODES
NUM_GPUS=$((GPUS_PER_NODE*SLURM_NNODES))

# so processes know who to talk to
MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
MASTER_PORT=6000


# Auto-generate the accelerate config
cat << EOT > $ACCELERATE_CONFIG_FILE
compute_environment: LOCAL_MACHINE
deepspeed_config: {}
distributed_type: MULTI_GPU
fsdp_config: {}
machine_rank: 0
main_process_ip: $MASTER_ADDR
main_process_port: $MASTER_PORT
main_training_function: main
num_machines: $SLURM_NNODES
num_processes: $NUM_GPUS
use_cpu: false
EOT


export MODEL_ID="stabilityai/stable-diffusion-xl-base-1.0"

PROGRAM="scripts/train_instruct_pix2pix_sdxl.py \
    --pretrained_model_name_or_path=$MODEL_ID \
    --pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix \
    --dataset_path='pipe:aws s3 cp s3://muse-datasets/instructpix2pix-clip-filtered-wds/{000000..000062}.tar -' \
    --use_ema \
    --enable_xformers_memory_efficient_attention \
    --resolution=1024 --random_flip \
    --conditioning_dropout_prob=0.05 \
    --per_gpu_batch_size=16 --gradient_accumulation_steps=1 \
    --num_workers=4 \
    --max_train_steps=20000 \
    --checkpointing_steps=2500 \
    --learning_rate=1e-5 --lr_warmup_steps=0 \
    --mixed_precision=fp16 \
    --val_image_url='https://hf.co/datasets/diffusers/diffusers-images-docs/resolve/main/mountain.png' \
    --validation_prompt='Turn sky into a cloudy one' \
    --seed=42 \
    --output_dir=$OUTPUT_DIR \
    --report_to=wandb \
    --push_to_hub
    "

# Note: it is important to escape `$SLURM_PROCID` since we want the srun on each node to evaluate this variable
export LAUNCHER="accelerate launch \
    --rdzv_conf "rdzv_backend=c10d,rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT,max_restarts=0,tee=3" \
    --config_file $ACCELERATE_CONFIG_FILE \
    --main_process_ip $MASTER_ADDR \
    --main_process_port $MASTER_PORT \
    --num_processes $NUM_GPUS \
    --machine_rank \$SLURM_PROCID \
    "


export CMD="$LAUNCHER $PROGRAM"
echo $CMD

# hide duplicated errors using this hack - will be properly fixed in pt-1.12
# export TORCHELASTIC_ERROR_FILE=/tmp/torch-elastic-error.json

# force crashing on nccl issues like hanging broadcast
export NCCL_ASYNC_ERROR_HANDLING=1
# export NCCL_DEBUG=INFO
# export NCCL_DEBUG_SUBSYS=COLL
# export NCCL_SOCKET_NTHREADS=1
# export NCCL_NSOCKS_PERTHREAD=1
# export CUDA_LAUNCH_BLOCKING=1

# AWS specific
export NCCL_PROTO=simple
export RDMAV_FORK_SAFE=1
export FI_EFA_FORK_SAFE=1
export FI_EFA_USE_DEVICE_RDMA=1
export FI_PROVIDER=efa
export FI_LOG_LEVEL=1
export NCCL_IB_DISABLE=1
export NCCL_SOCKET_IFNAME=ens


# srun error handling:
# --wait=60: wait 60 sec after the first task terminates before terminating all remaining tasks
# --kill-on-bad-exit=1: terminate a step if any task exits with a non-zero exit code
SRUN_ARGS=" \
    --wait=60 \
    --kill-on-bad-exit=1 \
    "

clear; srun $SRUN_ARGS --jobid $SLURM_JOB_ID bash -c "$CMD" 2>&1 | tee $LOG_PATH

echo "END TIME: $(date)"
